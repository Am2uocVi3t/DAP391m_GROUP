## IMPORT DATA FROM OPENMETEO AI


import numpy as np 
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

url = "vietnam_63provinces_ALL_VARS_weather_sample_20250531_20250607.csv"
df = pd.read_csv(url)
df.head()


## EDA - CHECKING DESCRIBE OF DATA


print(df.dtypes)
print(df.describe())
print(df.nunique())
# for col in df.columns:
#     print("Col: ", col)
#     print(df[col].unique())


df.isnull().sum()


## CHECKING OUTLINERS


from sklearn.preprocessing import StandardScaler

numeric_cols = df.select_dtypes(include=["float64", "int64"]).columns
scaler = StandardScaler()
df_scaled = pd.DataFrame(scaler.fit_transform(df[numeric_cols]), columns=numeric_cols)

plt.figure(figsize=(14,6))
sns.boxplot(data=df_scaled[numeric_cols])
plt.title("Boxplot of Normalized Features")
plt.xticks(rotation=90)
plt.show()


outlier_summary = {}

for col in numeric_cols:
    Q1 = df_scaled[col].quantile(0.25)
    Q3 = df_scaled[col].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    outliers = df_scaled[(df_scaled[col] < lower_bound) | (df_scaled[col] > upper_bound)]
    outlier_summary[col] = len(outliers)

# Hiển thị theo thứ tự giảm dần
outlier_summary = dict(sorted(outlier_summary.items(), key=lambda item: item[1], reverse=True))
print("Số lượng outliers theo từng cột:")
for col, count in outlier_summary.items():
    print(f"{col}: {count} outliers")



## REPLACE OUTLINERS WITH MEDIAN VALUES


def replace_outliers_with_median(df, cols):
    df_out = df.copy()
    for col in cols:
        Q1 = df_out[col].quantile(0.25)
        Q3 = df_out[col].quantile(0.75)
        IQR = Q3 - Q1
        lower_bound = Q1 - 1.5 * IQR
        upper_bound = Q3 + 1.5 * IQR

        # Tìm các outliers
        outliers = (df_out[col] < lower_bound) | (df_out[col] > upper_bound)

        # Thay thế bằng median
        median = df_out[col].median()
        df_out.loc[outliers, col] = median

    return df_out

df_scaled = replace_outliers_with_median(df_scaled, numeric_cols)


## VALUES REMAINING


outlier_counts = {}

for col in numeric_cols:
    Q1 = df_scaled[col].quantile(0.25)
    Q3 = df_scaled[col].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    outliers = df_scaled[(df_scaled[col] < lower_bound) | (df_scaled[col] > upper_bound)]
    outlier_counts[col] = outliers.shape[0]

outlier_summary = pd.DataFrame.from_dict(outlier_counts, orient='index', columns=['Outliers Remaining'])
outlier_summary = outlier_summary[outlier_summary['Outliers Remaining'] > 0]
display(outlier_summary)


## FEATURE "is_hot_day" WITH CONDITION "> 30"


df_scaled['is_hot_day'] = (df['temperature_2m'] > 35).astype(int)
print(df_scaled['is_hot_day'].value_counts())


## CHECKING CORR_MATRIX WITH > 0.6


corr_cols = numeric_cols.tolist() + ['is_hot_day']

corr = df_scaled[corr_cols].corr().abs()

plt.figure(figsize=(12, 8))
sns.heatmap(corr, annot=False, cmap='coolwarm')
plt.title("Correlation Matrix (normalized + label)")
plt.show()


high_corr = corr[corr > 0.8]
plt.figure(figsize=(12, 8))
sns.heatmap(high_corr, annot=False, cmap='coolwarm')
plt.title("Features with Correlation > 0.8")
plt.show()





## HISTOGRAM AFTER NORMALIZATION


df_scaled[numeric_cols].hist(bins=30, figsize=(30,10))
plt.suptitle("Histogram of Normalized Features")
plt.show()

plt.figure(figsize=(18,6))
sns.boxplot(data=df_scaled[numeric_cols])
plt.xticks(rotation=90)
plt.title("Boxplot after Normalization")
plt.show()


## 5 CLASSIFICATION MODELS


from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
from sklearn.model_selection import train_test_split

df_cls = df_scaled.copy()

selected_features = list(set(high_corr_features.index.tolist()))
if "is_hot_day" in selected_features:
    selected_features.remove("is_hot_day")
    

df_classification = df_cls[selected_features + ["is_hot_day"]].dropna()

X_cls = df_classification[selected_features]
y_cls = df_classification["is_hot_day"]

scaler = StandardScaler()
X_cls_scaled = scaler.fit_transform(X_cls)


X_train_cls, X_test_cls, y_train_cls, y_test_cls = train_test_split(
    X_cls_scaled, y_cls, test_size=0.2, random_state=42
)

classification_models = {
    "LogisticRegression": LogisticRegression(max_iter=1000),
    "RandomForest": RandomForestClassifier(random_state=42),
    "GradientBoosting": GradientBoostingClassifier(random_state=42),
    "DecisionTree": DecisionTreeClassifier(random_state=42),
    "SVC": SVC()
}

cls_results = []

for name, model in classification_models.items():
    model.fit(X_train_cls, y_train_cls)
    y_pred = model.predict(X_test_cls)
    acc = accuracy_score(y_test_cls, y_pred)
    prec = precision_score(y_test_cls, y_pred, zero_division=0)
    rec = recall_score(y_test_cls, y_pred)
    f1 = f1_score(y_test_cls, y_pred)
    cls_results.append((name, acc, prec, rec, f1))

cls_results_df = pd.DataFrame(cls_results, columns=["Model", "Accuracy", "Precision", "Recall", "F1 Score"])
cls_results_df.sort_values(by="F1 Score", ascending=False, inplace=True)
cls_results_df.reset_index(drop=True, inplace=True)
cls_results_df


print(y_test_cls[:10])
print(y_pred[:10])


## VISUAL FOR 5 CLASSIFICATION MODELS


for name, model in classification_models.items():
    model.fit(X_train_cls, y_train_cls)
    y_pred = model.predict(X_test_cls)
    cm = confusion_matrix(y_test_cls, y_pred)
    disp = ConfusionMatrixDisplay(confusion_matrix=cm)
    disp.plot()
    plt.title(f"Confusion Matrix: {name}")
    plt.show()


from sklearn.metrics import roc_auc_score, roc_curve

plt.figure(figsize=(10, 7))

for name, model in classification_models.items():
    if hasattr(model, "predict_proba"):
        y_score = model.predict_proba(X_test_cls)[:, 1]
    else:
        if hasattr(model, "decision_function"):
            y_score = model.decision_function(X_test_cls)
        else:
            continue
    fpr, tpr, _ = roc_curve(y_test_cls, y_score)
    auc = roc_auc_score(y_test_cls, y_score)

    plt.plot(fpr, tpr, label=f"{name} (AUC = {auc:.5f})")

plt.plot([0, 1], [0, 1], "k--", label="Random (AUC = 0.5)")
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("ROC Curve - AUC Score")
plt.legend(loc="lower right")
plt.grid(True)
plt.show()


## 5 REGRESSION MODELS


import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.svm import SVR


df_reg = df_scaled.copy()

target = 'temperature_2m'

corr_matrix = df_reg.corr(numeric_only=True)
corr_temp = corr_matrix[target].drop(target).sort_values(ascending=False)

selected_features = corr_temp[abs(corr_temp) > 0.6].index.tolist()

X = df_reg[selected_features]
y = df_reg[target]

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

models = {
    "LinearRegression": LinearRegression(),
    "RandomForest": RandomForestRegressor(random_state=42),
    "GradientBoosting": GradientBoostingRegressor(random_state=42),
    "DecisionTree": DecisionTreeRegressor(random_state=42),
    "SVR": SVR()
}

results = []

for name, model in models.items():
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    mae = mean_absolute_error(y_test, y_pred)
    mse = mean_squared_error(y_test, y_pred)
    r2 = r2_score(y_test, y_pred)
    results.append((name, mae, mse, r2))

results_df = pd.DataFrame(results, columns=["Model", "MAE", "MSE", "R^2"])
results_df.sort_values(by="R^2", ascending=False, inplace=True)
results_df.reset_index(drop=True, inplace=True)
results_df



## VISUAL 5 REGRESSION MODELS


palette = sns.color_palette("tab10", n_colors=len(results_df))

fig, axes = plt.subplots(1, 3, figsize=(18, 5))

sns.barplot(data=results_df, x='Model', y='MAE', hue='Model', palette='tab10', ax=axes[0], legend=False)
axes[0].set_title('Mean Absolute Error (MAE)')
axes[0].set_ylabel('MAE')
axes[0].set_xlabel('')
axes[0].tick_params(axis='x', rotation=45)

sns.barplot(data=results_df, x='Model', y='MSE', hue='Model', palette='tab10', ax=axes[1], legend=False)
axes[1].set_title('Mean Squared Error (MSE)')
axes[1].set_ylabel('MSE')
axes[1].set_xlabel('')
axes[1].tick_params(axis='x', rotation=45)

sns.barplot(data=results_df, x='Model', y='R^2', hue='Model', palette='tab10', ax=axes[2], legend=False)
axes[2].set_title('R-squared (R^2)')
axes[2].set_ylabel('R^2 Score')
axes[2].set_xlabel('')
axes[2].tick_params(axis='x', rotation=45)

plt.tight_layout()
plt.show()



